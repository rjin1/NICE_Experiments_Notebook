{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NICE_Experiment_Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/rjin1/NICE_Experiments_Notebook/blob/main/NICE_Experiment_Main.ipynb",
      "authorship_tag": "ABX9TyNd6HadAM1Bttr4zt7ZBEdS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjin1/NICE_Experiments_Notebook/blob/main/NICE_Experiment_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3g5QTcDmojA"
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.init as init\n",
        "import torch.nn as nn\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from scipy.io import savemat\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import argparse\n",
        "\n",
        "# np.set_printoptions(threshold=sys.maxsize)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueaSx5hBnNTG"
      },
      "source": [
        "def Gen_Source(loc_filename = './drive/MyDrive/NICE_JupyterNotebook/DistributionParams/GumbelLocatParams.mat', loc_varname = 'locat_param', \n",
        "               scale_filename = './drive/MyDrive/NICE_JupyterNotebook/DistributionParams/GumbelScaleParams.mat', sacle_varname = 'scale_param', \n",
        "               N_sample = 7668, seed_gs = 1):\n",
        "  \n",
        "  loc = loadmat(loc_filename)[loc_varname].astype(np.float64)\n",
        "  scale = loadmat(scale_filename)[sacle_varname].astype(np.float64)\n",
        "\n",
        "  # Assume the loc and scale are in same format np.array in (N_source x 1)\n",
        "  N_source = loc.shape[0]\n",
        "  S = np.zeros((N_source, N_sample), np.float64)\n",
        "\n",
        "  # Control the RNG for repro\n",
        "  np.random.seed(seed_gs)\n",
        "  for i in range(N_source):\n",
        "    S[i,:] = np.random.gumbel(loc[i,0], scale[i,0], (1, N_sample))\n",
        "\n",
        "  return S"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8XJwVpjn7hf"
      },
      "source": [
        "def Gen_Mixture(S, ratio=1):\n",
        "  # Assume S is in np.array format in (N_souece x N_sample)\n",
        "  N_source_sample = S.shape\n",
        "  X = np.zeros((N_source_sample[0], N_source_sample[1]), np.float64)\n",
        "\n",
        "  for i in range(N_source_sample[1]):\n",
        "    for j in range(N_source_sample[0]):\n",
        "      X[j,i] = S[j,i] + ratio * (S[j,i] * (np.sum(S[:,i]) - S[j,i]) + np.sum(S[:,i] ** 2) - S[j,i] ** 2)\n",
        "\n",
        "  return X "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHOE7GqJPxf"
      },
      "source": [
        "def Train_Valid_Split(Data, N_train, N_valid, seed_tvs):\n",
        "  # Assume data in format np.array with (N_source x N_sample)\n",
        "  N_sample = Data.shape[1]\n",
        "  \n",
        "  np.random.seed(seed_tvs)\n",
        "  ind_all = np.random.permutation(N_sample)\n",
        "  ind_train = ind_all[:N_train]\n",
        "  ind_valid = ind_all[N_train:N_train+N_valid]\n",
        "\n",
        "  Data_train = Data[:,ind_train]\n",
        "  Data_valid = Data[:,ind_valid]\n",
        "\n",
        "  return Data_train, Data_valid\n",
        "  "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU35G8Tbhkko"
      },
      "source": [
        "def load_fMRI_regression(Data_in, batch_size=10, CUDA=True, if_shuffle=False):\n",
        "    return data.DataLoader(\n",
        "        LOAD_FMRI_regression(Data_in),\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=CUDA,\n",
        "        shuffle=if_shuffle\n",
        "    )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNIOeL2lCbt"
      },
      "source": [
        "class LOAD_FMRI_regression(Dataset):\n",
        "    def __init__(self, Data_in):\n",
        "        # Input training, validation or test data sets as Data_in below to make this class a corresponding loader.\n",
        "        self.Data_in = Data_in\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Data_in)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.Data_in[idx]\n",
        "        return sample\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQCqdDerTS9"
      },
      "source": [
        "# Implementation of models from paper.\n",
        "def _build_relu_network1(hidden_dim, dropout_p):\n",
        "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "def _build_relu_network2(hidden_dim, dropout_p):\n",
        "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "def _build_relu_network3(hidden_dim, dropout_p):\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "# def _build_relu_network4(hidden_dim, dropout_p):\n",
        "    # \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    # _modules = [nn.Dropout(p=dropout_p)]\n",
        "    # _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    # _modules.append(nn.ReLU())\n",
        "    # _modules.append(nn.Dropout(p=dropout_p))\n",
        "    # _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    # return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "#\n",
        "# def _build_relu_network5(hidden_dim, dropout_p):\n",
        "#     \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "#     _modules = [nn.Dropout(p=dropout_p)]\n",
        "#     _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "#     _modules.append(nn.ReLU())\n",
        "#     _modules.append(nn.Dropout(p=dropout_p))\n",
        "#     _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "#     return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "# def _build_relu_network6(hidden_dim, dropout_p):\n",
        "#     \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "#     _modules = [nn.Dropout(p=dropout_p)]\n",
        "#     _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "#     _modules.append(nn.ReLU())\n",
        "#     _modules.append(nn.Dropout(p=dropout_p))\n",
        "#     _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "#     return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "class NICEModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Replication of model from the paper:\n",
        "      \"Nonlinear Independent Components Estimation\",\n",
        "      Laurent Dinh, David Krueger, Yoshua Bengio (2014)\n",
        "      https://arxiv.org/abs/1410.8516\n",
        "\n",
        "    Contains the following components:\n",
        "    * four additive coupling layers with nonlinearity functions consisting of\n",
        "      five-layer RELUs\n",
        "    * a diagonal scaling matrix output layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, init_interval, dropout_p, init_filepath):\n",
        "        # def __init__(self, input_dim, hidden_dim, init_interval, dropout_p, init_filepath, param1, param2):\n",
        "        super(NICEModel, self).__init__()\n",
        "        assert (input_dim % 2 == 0), \"[NICEModel] only even input dimensions supported for now\"\n",
        "        self.input_dim = input_dim\n",
        "        half_dim = int(input_dim / 2)\n",
        "        self.layer1 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network1(hidden_dim, dropout_p))\n",
        "        self.layer2 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network2(hidden_dim, dropout_p))\n",
        "        self.layer3 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network3(hidden_dim, dropout_p))\n",
        "        # self.layer4 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network4(hidden_dim, dropout_p))\n",
        "        # self.layer5 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network5(hidden_dim, dropout_p))\n",
        "        # self.layer6 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network6(hidden_dim, dropout_p))\n",
        "        # Rui: Initialize scaling layer parameters.\n",
        "        self.scaling_diag = nn.Parameter(torch.zeros(input_dim), requires_grad=True)\n",
        "        # self.param1 = nn.Parameter(torch.tensor(param1), requires_grad=True)\n",
        "        # self.param2 = nn.Parameter(torch.tensor(param2), requires_grad=True)\n",
        "        # self.scaling_diag = nn.Parameter(torch.tensor(loadmat(init_filepath)['scaling_diag'].astype(np.float64)[0][:])\n",
        "        #                                  , requires_grad=True)\n",
        "        # randomly initialize weights:\n",
        "        # ind = 1\n",
        "        for p in self.layer1.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer1nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer1nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "                init.zeros_(p)\n",
        "        # ind = 1\n",
        "        for p in self.layer2.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer2nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                init.zeros_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer2nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "        # ind = 1\n",
        "        for p in self.layer3.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer3nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer3nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "                init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer4.parameters():\n",
        "        #     if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "        #         init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer4nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "        #     else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer4nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "        #         init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer5.parameters():\n",
        "        #     if len(p.shape) > 1:\n",
        "        #         # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "        #         init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer5nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "        #     else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer5nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "        #         init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer6.parameters():\n",
        "        #     if len(p.shape) > 1:\n",
        "        #         # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "        #         init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer6nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "        #     else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer6nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "        #         init.zeros_(p)\n",
        "        \n",
        "\n",
        "    def forward(self, xs):\n",
        "        \"\"\"\n",
        "        Forward pass through all invertible coupling layers.\n",
        "\n",
        "        Args:\n",
        "        * xs: float tensor of shape (B,dim).\n",
        "\n",
        "        Returns:\n",
        "        * ys: float tensor of shape (B,dim).\n",
        "        \"\"\"\n",
        "        ys = self.layer1(xs)\n",
        "        ys = self.layer2(ys)\n",
        "        ys = self.layer3(ys)\n",
        "        # ys = self.layer4(ys)\n",
        "        # ys = self.layer5(ys)\n",
        "        # ys = self.layer6(ys)\n",
        "        # ys = self.layer7(ys)\n",
        "        ys_scale = torch.matmul(ys, torch.diag(torch.exp(self.scaling_diag)))\n",
        "        return ys_scale, ys\n",
        "        \n",
        "    def inverse(self, ys):\n",
        "        \"\"\"Invert a set of draws from gaussians\"\"\"\n",
        "        with torch.no_grad():\n",
        "            xs = torch.matmul(ys, torch.diag(torch.reciprocal(torch.exp(self.scaling_diag))))\n",
        "            xs = self.layer4.inverse(xs)\n",
        "            xs = self.layer3.inverse(xs)\n",
        "            xs = self.layer2.inverse(xs)\n",
        "            xs = self.layer1.inverse(xs)\n",
        "        return xs\n",
        "\n",
        "# Implementation of NICE bijective triangular-jacobian layers.\n",
        "# ===== ===== Coupling Layer Implementations ===== =====\n",
        "#\n",
        "# _get_even = lambda xs: xs[:,::2]\n",
        "# _get_odd = lambda xs: xs[:,1::2]\n",
        "_get_even = lambda xs: xs[:, 8:]\n",
        "_get_odd = lambda xs: xs[:, :8]\n",
        "# _get_even = lambda xs: xs[:, 1:]\n",
        "# _get_odd = lambda xs: xs[:, :1]\n",
        "\n",
        "\n",
        "def _interleave(first, second, order):\n",
        "    \"\"\"\n",
        "    Given 2 rank-2 tensors with same batch dimension, interleave their columns.\n",
        "    \n",
        "    The tensors \"first\" and \"second\" are assumed to be of shape (B,M) and (B,N)\n",
        "    where M = N or N+1, repsectively.\n",
        "    \"\"\"\n",
        "    cols = []\n",
        "    # if order == 'even':\n",
        "    #     for k in range(second.shape[1]):\n",
        "    #         cols.append(first[:,k])\n",
        "    #         cols.append(second[:,k])\n",
        "    #     if first.shape[1] > second.shape[1]:\n",
        "    #         cols.append(first[:,-1])\n",
        "    # else:\n",
        "    #     for k in range(first.shape[1]):\n",
        "    #         cols.append(second[:,k])\n",
        "    #         cols.append(first[:,k])\n",
        "    #     if second.shape[1] > first.shape[1]:\n",
        "    #         cols.append(second[:,-1])\n",
        "    if order == 'even':\n",
        "         cols.append(second)\n",
        "         cols.append(first)\n",
        "    else:\n",
        "         cols.append(first)\n",
        "         cols.append(second)\n",
        "    # return torch.stack(cols, dim=1)\n",
        "    return torch.cat(cols, dim=1)\n",
        "\n",
        "\n",
        "class _BaseCouplingLayer(nn.Module):\n",
        "    def __init__(self, dim, partition, nonlinearity):\n",
        "        \"\"\"\n",
        "        Base coupling layer that handles the permutation of the inputs and wraps\n",
        "        an instance of torch.nn.Module.\n",
        "\n",
        "        Usage:\n",
        "        >> layer = AdditiveCouplingLayer(1000, 'even', nn.Sequential(...))\n",
        "        \n",
        "        Args:\n",
        "        * dim: dimension of the inputs.\n",
        "        * partition: str, 'even' or 'odd'. If 'even', the even-valued columns are sent to\n",
        "        pass through the activation module.\n",
        "        * nonlinearity: an instance of torch.nn.Module.\n",
        "        \"\"\"\n",
        "        super(_BaseCouplingLayer, self).__init__()\n",
        "        # store input dimension of incoming values:\n",
        "        self.dim = dim\n",
        "        # store partition choice and make shorthands for 1st and second partitions:\n",
        "        assert (partition in ['even', 'odd']), \"[_BaseCouplingLayer] Partition type must be `even` or `odd`!\"\n",
        "        self.partition = partition\n",
        "        if (partition == 'even'):\n",
        "            self._first = _get_even\n",
        "            self._second = _get_odd\n",
        "        else:\n",
        "            self._first = _get_odd\n",
        "            self._second = _get_even\n",
        "        # store nonlinear function module:\n",
        "        # (n.b. this can be a complex instance of torch.nn.Module, for ex. a deep ReLU network)\n",
        "        self.add_module('nonlinearity', nonlinearity)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Map an input through the partition and nonlinearity.\"\"\"\n",
        "        return _interleave(\n",
        "            self._first(x),\n",
        "            self.coupling_law(self._second(x), self.nonlinearity(self._first(x))),\n",
        "            self.partition\n",
        "        )\n",
        "\n",
        "    def inverse(self, y):\n",
        "        \"\"\"Inverse mapping through the layer. Gradients should be turned off for this pass.\"\"\"\n",
        "        return _interleave(\n",
        "            self._first(y),\n",
        "            self.anticoupling_law(self._second(y), self.nonlinearity(self._first(y))),\n",
        "            self.partition\n",
        "        )\n",
        "\n",
        "    def coupling_law(self, a, b):\n",
        "        # (a,b) --> g(a,b)\n",
        "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
        "\n",
        "    def anticoupling_law(self, a, b):\n",
        "        # (a,b) --> g^{-1}(a,b)\n",
        "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
        "\n",
        "\n",
        "class AdditiveCouplingLayer(_BaseCouplingLayer):\n",
        "    \"\"\"Layer with coupling law g(a;b) := a + b.\"\"\"\n",
        "    def coupling_law(self, a, b):\n",
        "        return (a + b)\n",
        "    def anticoupling_law(self, a, b):\n",
        "        return (a - b)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ8j5GO8228e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIDZwkfbgfcU"
      },
      "source": [
        "def train(args, Data_train, Data_valid, Data_test, DEVICE, CUDA):\n",
        "  \"\"\"Construct a NICE model and train over a number of epochs.\"\"\"\n",
        "    # === choose which dataset to build:\n",
        "    # global early_stop_account_start\n",
        "  if args.dataset == 'fMRI':\n",
        "    dataloader_fn = load_fMRI_regression\n",
        "    input_dim = Data_train.shape[0]\n",
        "    # Loss\n",
        "    LOSS_train = np.array([], dtype=np.float64)\n",
        "    LOSS_val = np.array([], dtype=np.float64)\n",
        "    LOSS_val_epoch = np.array([], dtype=np.float64)\n",
        "    LR = np.array([], dtype=np.float64)\n",
        "    # Mean Pearson correlation.-Rui\n",
        "    MPC = np.array([], dtype=np.float64)\n",
        "    # Mean square error.-Rui\n",
        "    MSE = np.array([], dtype=np.float64)\n",
        "    MSE_scaled = np.array([], dtype=np.float64)\n",
        "    params_1 = loadmat(args.param1fp)[args.param1vn].transpose().astype(np.float64)\n",
        "    params_2 = loadmat(args.param2fp)[args.param2vn].transpose().astype(np.float64)\n",
        "\n",
        "  model = NICEModel(input_dim, args.nhidden, args.init_interval, args.dropout_p, args.init_filepath)\n",
        "  if (args.model_path is not None):\n",
        "      assert (os.path.exists(args.model_path)), \"[train] model does not exist at specified location\"\n",
        "      model.load_state_dict(torch.load(args.model_path, map_location='cpu'))\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  opt = optim.RMSprop(model.parameters(), lr=args.lr, momentum=0.9)\n",
        "  # opt = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "  # === choose which loss function to build:\n",
        "  if args.prior == 'logistic':\n",
        "      nice_loss_fn = LogisticPriorNICELoss(size_average=True)\n",
        "  if args.prior == 'Gumbel':\n",
        "      nice_loss_fn = GumbelPriorNICELoss(DEVICE, size_average=False)\n",
        "  if args.prior == 'prior':\n",
        "      nice_loss_fn = GaussianPriorNICELoss(size_average=True)\n",
        "  if args.prior == 'regression':\n",
        "      nice_loss_fn = regression(DEVICE)\n",
        "\n",
        "  return 'Training is done'"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2wBGM-mX34H"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # ----- parse training settings:\n",
        "    parser = argparse.ArgumentParser(description=\"Train a fresh NICE model and save.\")\n",
        "    # configuration settings:\n",
        "    parser.add_argument(\"--dataset\", default='fMRI', dest='dataset', choices=('tfd', 'cifar10', 'svhn', 'mnist'),\n",
        "                        help=\"Dataset to train the NICE model on.\")\n",
        "    parser.add_argument(\"--epochs\", dest='num_epochs', default=40000, type=int,\n",
        "                        help=\"Number of epochs to train on. [1500]\")\n",
        "    parser.add_argument(\"--batch_size\", dest=\"batch_size_train\", default=10, type=int,\n",
        "                        help=\"Number of training examples per batch. [16]\")\n",
        "    parser.add_argument(\"--Train_size\", dest=\"size_train\", default=5110, type=int,\n",
        "                        help=\"Number of training examples. [5110]\")\n",
        "    parser.add_argument(\"--Validation_size\", dest=\"size_valid\", default=2558, type=int,\n",
        "                        help=\"Number of validation examples. [2558]\")\n",
        "    parser.add_argument(\"--savedir\", dest='savedir',\n",
        "                        default=\"./drive/MyDrive/NICE_JupyterNotebook/Results/\",\n",
        "                        help=\"Where to save the results and trained models.\")\n",
        "    parser.add_argument(\"--param1filepath\", dest='param1fp',\n",
        "                        default=\"./drive/MyDrive/NICE_JupyterNotebook/DistributionParams/GumbelLocatParams.mat\",\n",
        "                        help=\"Parameter 1 file path.\")\n",
        "    parser.add_argument(\"--param1variablename\", dest='param1vn',\n",
        "                        default=\"locat_param\",\n",
        "                        help=\"Parameter 1 variable name.\")\n",
        "    parser.add_argument(\"--param2filepath\", dest='param2fp',\n",
        "                        default=\"./drive/MyDrive/NICE_JupyterNotebook/DistributionParams/GumbelScaleParams.mat\",\n",
        "                        help=\"Parameter 2 file path.\")\n",
        "    parser.add_argument(\"--param2variablename\", dest='param2vn',\n",
        "                        default=\"scale_param\",\n",
        "                        help=\"Parameter 2 variable name.\")\n",
        "    parser.add_argument(\"--initialization_path\", dest='init_filepath',\n",
        "                        default=\"./fMRI_data/\",\n",
        "                        help=\"Where to load the pretrained model parameters.\")\n",
        "\n",
        "    # validation and test:\n",
        "    parser.add_argument(\"--val_batch_size\", dest=\"batch_size_val\", default=2558, type=int,\n",
        "                        help=\"Number of validation examples per batch. [16]\")\n",
        "    parser.add_argument(\"--test_batch_size\", dest=\"batch_size_test\", default=7668, type=int,\n",
        "                        help=\"Number of test examples per batch. [16]\")\n",
        "    parser.add_argument(\"--early_stop_iteration\", dest=\"early_stop_iter\", default=20, type=int,\n",
        "                        help=\"Number of iterations for early stopping. [16]\")\n",
        "\n",
        "    # model settings:\n",
        "    parser.add_argument(\"--nonlinearity_hiddens\", dest='nhidden', default=2, type=int,\n",
        "                        help=\"Hidden size of inner layers of nonlinearity. [1000]\")\n",
        "    parser.add_argument(\"--nonlinearity_dropout\", dest='dropout_p', default=0.0, type=float,\n",
        "                        help=\"The dropout probability in each layer (except scaling layer). [0.8]\")\n",
        "    parser.add_argument(\"--prior\", choices=('logistic', 'Gumbel', 'prior', 'regression'), default='Gumbel',\n",
        "                        help=\"Prior distribution of latent space components. [logistic]\")\n",
        "    parser.add_argument(\"--model_path\", dest='model_path', default=None, type=str,\n",
        "                        help=\"Continue from pretrained model. [None]\")\n",
        "    parser.add_argument(\"--uniform_init_interval\", dest='init_interval', default=1e-2, type=float,\n",
        "                        help=\"The interval of uniform initialization. [0.01]\")\n",
        "\n",
        "    # optimization settings:\n",
        "    parser.add_argument(\"--lr\", default=1e-3, dest='lr', type=float,\n",
        "                        help=\"Learning rate for ADAM optimizer. [0.001]\")\n",
        "    parser.add_argument(\"--beta1\", default=0.9, dest='beta1', type=float,\n",
        "                        help=\"Momentum for ADAM optimizer. [0.9]\")\n",
        "    parser.add_argument(\"--beta2\", default=0.999, dest='beta2', type=float,\n",
        "                        help=\"Beta2 for ADAM optimizer. [0.01]\")\n",
        "    parser.add_argument(\"--eps\", default=1e-8, dest='eps', type=float,\n",
        "                        help=\"Epsilon for ADAM optimizer. [0.0001]\")\n",
        "    parser.add_argument(\"--lambda\", default=0.0, dest='lmbda', type=float,\n",
        "                        help=\"L1 weight decay coefficient. [0.0]\")\n",
        "    parser.add_argument(\"--DataRNGseed\", dest=\"DataRNG_seed\", default=1, type=int,\n",
        "                        help=\"Data random number generator seed. [1]\")\n",
        "    parser.add_argument(\"--ModelRNGseed\", dest=\"ModelRNG_seed\", default=1, type=int,\n",
        "                        help=\"Model random number generator seed. [1]\")\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "718wvRSXbXdV",
        "outputId": "fb2ef88c-fdbe-49cc-d171-e857562d9300"
      },
      "source": [
        "    # ----- run training loop over several epochs & save models for each epoch:\n",
        "    # RNG control\n",
        "    # torch.manual_seed(args.RNG_seed)\n",
        "    # random.seed(args.DataRNG_seed)\n",
        "    # np.random.seed(args.RNG_seed)\n",
        "    torch.set_default_dtype(torch.double)\n",
        "    \n",
        "    # Data generation and split\n",
        "    Sources = Gen_Source(loc_filename=args.param1fp, loc_varname=args.param1vn, \n",
        "                         scale_filename=args.param2fp, sacle_varname=args.param2vn,\n",
        "                         N_sample=args.size_train+args.size_valid, seed_gs=args.DataRNG_seed)\n",
        "    Data = Gen_Mixture(Sources, ratio=1)\n",
        "    Data_train, Data_valid = Train_Valid_Split(Data, args.size_train, args.size_valid, args.DataRNG_seed)\n",
        "\n",
        "    # CUDA check\n",
        "    if torch.cuda.is_available():\n",
        "      DEVICE = torch.device('cuda:0')\n",
        "      CUDA = True\n",
        "    else:\n",
        "      DEVICE = torch.device('cpu')\n",
        "      CUDA = False\n",
        "    \n",
        "    print('Training device:'+DEVICE.type+'\\n')\n",
        "    train(args, Data_train, Data_valid, Data, DEVICE, CUDA)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training device:cuda\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Training is done'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}