{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NICE_SynfMRIspatialmapsExperiment_Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/rjin1/NICE_Experiments_Notebook/blob/main/NICE_SynfMRIspatialmapsExperiment_Main.ipynb",
      "authorship_tag": "ABX9TyN8r1nyC17HPHNviEhGOFyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjin1/NICE_Experiments_Notebook/blob/main/NICE_SynfMRIspatialmapsExperiment_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3g5QTcDmojA"
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.init as init\n",
        "import torch.nn as nn\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from scipy.io import savemat\n",
        "import scipy.stats as scistats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import argparse\n",
        "\n",
        "# np.set_printoptions(threshold=sys.maxsize)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueaSx5hBnNTG"
      },
      "source": [
        "def Gen_Source(loc_filename = './drive/MyDrive/NICE_JupyterNotebook/DistributionParams/SM_params_locat.mat', loc_varname = 'locat_param', \n",
        "               scale_filename = './drive/MyDrive/NICE_JupyterNotebook/DistributionParams/SM_params_scale.mat', sacle_varname = 'scale_param', N_sample = 7668, seed_gs = 1):\n",
        "  \n",
        "  loc = loadmat(loc_filename)[loc_varname].astype(np.float64)\n",
        "  scale = loadmat(scale_filename)[sacle_varname].astype(np.float64)\n",
        "\n",
        "  # Assume the loc and scale are in same format np.array in (N_source x 1)\n",
        "  N_source = loc.shape[0]\n",
        "  S = np.zeros((N_source, N_sample), np.float64)\n",
        "\n",
        "  # Control the RNG for repro\n",
        "  np.random.seed(seed_gs)\n",
        "  for i in range(N_source):\n",
        "    S[i,:] = np.random.gumbel(loc[i,0], scale[i,0], (1, N_sample))\n",
        "\n",
        "  return S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8XJwVpjn7hf"
      },
      "source": [
        "def Gen_Mixture(S, ratio=1):\n",
        "  # Assume S is in np.array format in (N_souece x N_sample)\n",
        "  N_source_sample = S.shape\n",
        "  X = np.zeros((N_source_sample[0], N_source_sample[1]), np.float64)\n",
        "\n",
        "  for i in range(N_source_sample[1]):\n",
        "    for j in range(N_source_sample[0]):\n",
        "      X[j,i] = S[j,i] + ratio * (S[j,i] * (np.sum(S[:,i]) - S[j,i]) + np.sum(S[:,i] ** 2) - S[j,i] ** 2)\n",
        "\n",
        "  return X "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHOE7GqJPxf"
      },
      "source": [
        "def Train_Valid_Split(Data, N_train, N_valid, seed_tvs):\n",
        "  # Assume data in format np.array with (N_source x N_sample)\n",
        "  N_sample = Data.shape[1]\n",
        "  \n",
        "  np.random.seed(seed_tvs)\n",
        "  ind_all = np.random.permutation(N_sample)\n",
        "  ind_train = ind_all[:N_train]\n",
        "  ind_valid = ind_all[N_train:N_train+N_valid]\n",
        "\n",
        "  Data_train = Data[:,ind_train]\n",
        "  Data_valid = Data[:,ind_valid]\n",
        "\n",
        "  return Data_train, Data_valid\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vIPr2OC9xuI"
      },
      "source": [
        "def Add_Noise(Data, SNR, seed):\n",
        "  # Add Gaussian noise per mixture with SNR\n",
        "  np.random.seed(seed)\n",
        "  N_source_sample = Data.shape\n",
        "  Noisy_Data = np.zeros(N_source_sample, np.float64)\n",
        "  \n",
        "  for i in range(N_source_sample[0]):\n",
        "    Noisy_Data[i,:] = Data[i,:] + (np.std(Data[i,:]) / np.sqrt(SNR)) * np.random.randn(N_source_sample[1])\n",
        "\n",
        "  return Noisy_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU35G8Tbhkko"
      },
      "source": [
        "def load_fMRI_regression(Data_in, batch_size=10, if_CUDA=True, if_shuffle=False):\n",
        "    return data.DataLoader(\n",
        "        LOAD_FMRI_regression(Data_in),\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=if_CUDA,\n",
        "        shuffle=if_shuffle\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNIOeL2lCbt"
      },
      "source": [
        "class LOAD_FMRI_regression(Dataset):\n",
        "    def __init__(self, Data_in):\n",
        "        # Input training, validation or test data sets as Data_in below to make this class a corresponding loader.\n",
        "        self.Data_in = Data_in\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Data_in)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.Data_in[idx]\n",
        "        return sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQCqdDerTS9"
      },
      "source": [
        "# Implementation of models from paper.\n",
        "def _build_relu_network1(hidden_dim, dropout_p):\n",
        "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "def _build_relu_network2(hidden_dim, dropout_p):\n",
        "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "def _build_relu_network3(hidden_dim, dropout_p):\n",
        "    _modules = [nn.Dropout(p=dropout_p)]\n",
        "    _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    _modules.append(nn.ReLU())\n",
        "    _modules.append(nn.Dropout(p=dropout_p))\n",
        "    _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "# def _build_relu_network4(hidden_dim, dropout_p):\n",
        "    # \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    # _modules = [nn.Dropout(p=dropout_p)]\n",
        "    # _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    # _modules.append(nn.ReLU())\n",
        "    # _modules.append(nn.Dropout(p=dropout_p))\n",
        "    # _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    # return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "#\n",
        "# def _build_relu_network5(hidden_dim, dropout_p):\n",
        "    # \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "    # _modules = [nn.Dropout(p=dropout_p)]\n",
        "    # _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "    # _modules.append(nn.ReLU())\n",
        "    # _modules.append(nn.Dropout(p=dropout_p))\n",
        "    # _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "    # return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "# def _build_relu_network6(hidden_dim, dropout_p):\n",
        "#     \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
        "#     _modules = [nn.Dropout(p=dropout_p)]\n",
        "#     _modules.append(nn.Linear(8, hidden_dim, bias=True))\n",
        "#     _modules.append(nn.ReLU())\n",
        "#     _modules.append(nn.Dropout(p=dropout_p))\n",
        "#     _modules.append(nn.Linear(hidden_dim, 8, bias=True))\n",
        "#     return nn.Sequential(*_modules)\n",
        "\n",
        "\n",
        "class NICEModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Replication of model from the paper:\n",
        "      \"Nonlinear Independent Components Estimation\",\n",
        "      Laurent Dinh, David Krueger, Yoshua Bengio (2014)\n",
        "      https://arxiv.org/abs/1410.8516\n",
        "\n",
        "    Contains the following components:\n",
        "    * four additive coupling layers with nonlinearity functions consisting of\n",
        "      five-layer RELUs\n",
        "    * a diagonal scaling matrix output layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, init_interval, dropout_p, init_filepath, seed):\n",
        "        # def __init__(self, input_dim, hidden_dim, init_interval, dropout_p, init_filepath, param1, param2):\n",
        "        super(NICEModel, self).__init__()\n",
        "        assert (input_dim % 2 == 0), \"[NICEModel] only even input dimensions supported for now\"\n",
        "        self.input_dim = input_dim\n",
        "        half_dim = int(input_dim / 2)\n",
        "        self.layer1 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network1(hidden_dim, dropout_p))\n",
        "        self.layer2 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network2(hidden_dim, dropout_p))\n",
        "        self.layer3 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network3(hidden_dim, dropout_p))\n",
        "        # self.layer4 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network4(hidden_dim, dropout_p))\n",
        "        # self.layer5 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network5(hidden_dim, dropout_p))\n",
        "        # self.layer6 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network6(hidden_dim, dropout_p))\n",
        "        # Rui: Initialize scaling layer parameters.\n",
        "        self.scaling_diag = nn.Parameter(torch.zeros(input_dim), requires_grad=True)\n",
        "        # self.param1 = nn.Parameter(torch.tensor(param1), requires_grad=True)\n",
        "        # self.param2 = nn.Parameter(torch.tensor(param2), requires_grad=True)\n",
        "        # self.scaling_diag = nn.Parameter(torch.tensor(loadmat(init_filepath)['scaling_diag'].astype(np.float64)[0][:])\n",
        "        #                                  , requires_grad=True)\n",
        "        # randomly initialize weights:\n",
        "        # RNG control   \n",
        "        torch.manual_seed(seed)\n",
        "        # ind = 1\n",
        "        for p in self.layer1.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer1nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer1nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "                init.zeros_(p)\n",
        "        # ind = 1\n",
        "        for p in self.layer2.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer2nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                init.zeros_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer2nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "        # ind = 1\n",
        "        for p in self.layer3.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.normal_(p, mean=0., std=0.1)\n",
        "                init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer3nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "                # ind = ind + 1\n",
        "            else:\n",
        "                # init.normal_(p)\n",
        "                # with torch.no_grad():\n",
        "                #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "                #         loadmat(init_filepath)['layer3nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "                # ind = ind + 1\n",
        "                init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer4.parameters():\n",
        "            # if len(p.shape) > 1:\n",
        "                # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer4nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "            # else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer4nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "                # init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer5.parameters():\n",
        "            # if len(p.shape) > 1:\n",
        "        #         # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "                # init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer5nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "            # else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer5nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "                # init.zeros_(p)\n",
        "        # ind = 1\n",
        "        # for p in self.layer6.parameters():\n",
        "        #     if len(p.shape) > 1:\n",
        "        #         # init.kaiming_uniform_(p, nonlinearity='relu')\n",
        "        #         init.uniform_(p, a=-init_interval, b=init_interval)\n",
        "        #         # init.normal_(p, mean=0., std=0.1)\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer6nonlinearityweight_'+str(ind)].astype(np.float64)))\n",
        "        #         # ind = ind + 1\n",
        "        #     else:\n",
        "        #         # with torch.no_grad():\n",
        "        #         #     p.copy_(torch.squeeze(torch.from_numpy(\n",
        "        #         #         loadmat(init_filepath)['layer6nonlinearitybias_'+str(ind)].astype(np.float64))))\n",
        "        #         # ind = ind + 1\n",
        "        #         init.zeros_(p)\n",
        "        \n",
        "\n",
        "    def forward(self, xs):\n",
        "        \"\"\"\n",
        "        Forward pass through all invertible coupling layers.\n",
        "\n",
        "        Args:\n",
        "        * xs: float tensor of shape (B,dim).\n",
        "\n",
        "        Returns:\n",
        "        * ys: float tensor of shape (B,dim).\n",
        "        \"\"\"\n",
        "        ys = self.layer1(xs)\n",
        "        ys = self.layer2(ys)\n",
        "        ys = self.layer3(ys)\n",
        "        # ys = self.layer4(ys)\n",
        "        # ys = self.layer5(ys)\n",
        "        # ys = self.layer6(ys)\n",
        "        # ys = self.layer7(ys)\n",
        "        ys_scale = torch.matmul(ys, torch.diag(torch.exp(self.scaling_diag)))\n",
        "        return ys_scale, ys\n",
        "        \n",
        "    def inverse(self, ys):\n",
        "        \"\"\"Invert a set of draws from gaussians\"\"\"\n",
        "        with torch.no_grad():\n",
        "            xs = torch.matmul(ys, torch.diag(torch.reciprocal(torch.exp(self.scaling_diag))))\n",
        "            xs = self.layer4.inverse(xs)\n",
        "            xs = self.layer3.inverse(xs)\n",
        "            xs = self.layer2.inverse(xs)\n",
        "            xs = self.layer1.inverse(xs)\n",
        "        return xs\n",
        "\n",
        "# Implementation of NICE bijective triangular-jacobian layers.\n",
        "# ===== ===== Coupling Layer Implementations ===== =====\n",
        "#\n",
        "# _get_even = lambda xs: xs[:,::2]\n",
        "# _get_odd = lambda xs: xs[:,1::2]\n",
        "_get_even = lambda xs: xs[:, 8:]\n",
        "_get_odd = lambda xs: xs[:, :8]\n",
        "# _get_even = lambda xs: xs[:, 1:]\n",
        "# _get_odd = lambda xs: xs[:, :1]\n",
        "\n",
        "\n",
        "def _interleave(first, second, order):\n",
        "    \"\"\"\n",
        "    Given 2 rank-2 tensors with same batch dimension, interleave their columns.\n",
        "    \n",
        "    The tensors \"first\" and \"second\" are assumed to be of shape (B,M) and (B,N)\n",
        "    where M = N or N+1, repsectively.\n",
        "    \"\"\"\n",
        "    cols = []\n",
        "    # if order == 'even':\n",
        "    #     for k in range(second.shape[1]):\n",
        "    #         cols.append(first[:,k])\n",
        "    #         cols.append(second[:,k])\n",
        "    #     if first.shape[1] > second.shape[1]:\n",
        "    #         cols.append(first[:,-1])\n",
        "    # else:\n",
        "    #     for k in range(first.shape[1]):\n",
        "    #         cols.append(second[:,k])\n",
        "    #         cols.append(first[:,k])\n",
        "    #     if second.shape[1] > first.shape[1]:\n",
        "    #         cols.append(second[:,-1])\n",
        "    if order == 'even':\n",
        "         cols.append(second)\n",
        "         cols.append(first)\n",
        "    else:\n",
        "         cols.append(first)\n",
        "         cols.append(second)\n",
        "    # return torch.stack(cols, dim=1)\n",
        "    return torch.cat(cols, dim=1)\n",
        "\n",
        "\n",
        "class _BaseCouplingLayer(nn.Module):\n",
        "    def __init__(self, dim, partition, nonlinearity):\n",
        "        \"\"\"\n",
        "        Base coupling layer that handles the permutation of the inputs and wraps\n",
        "        an instance of torch.nn.Module.\n",
        "\n",
        "        Usage:\n",
        "        >> layer = AdditiveCouplingLayer(1000, 'even', nn.Sequential(...))\n",
        "        \n",
        "        Args:\n",
        "        * dim: dimension of the inputs.\n",
        "        * partition: str, 'even' or 'odd'. If 'even', the even-valued columns are sent to\n",
        "        pass through the activation module.\n",
        "        * nonlinearity: an instance of torch.nn.Module.\n",
        "        \"\"\"\n",
        "        super(_BaseCouplingLayer, self).__init__()\n",
        "        # store input dimension of incoming values:\n",
        "        self.dim = dim\n",
        "        # store partition choice and make shorthands for 1st and second partitions:\n",
        "        assert (partition in ['even', 'odd']), \"[_BaseCouplingLayer] Partition type must be `even` or `odd`!\"\n",
        "        self.partition = partition\n",
        "        if (partition == 'even'):\n",
        "            self._first = _get_even\n",
        "            self._second = _get_odd\n",
        "        else:\n",
        "            self._first = _get_odd\n",
        "            self._second = _get_even\n",
        "        # store nonlinear function module:\n",
        "        # (n.b. this can be a complex instance of torch.nn.Module, for ex. a deep ReLU network)\n",
        "        self.add_module('nonlinearity', nonlinearity)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Map an input through the partition and nonlinearity.\"\"\"\n",
        "        return _interleave(\n",
        "            self._first(x),\n",
        "            self.coupling_law(self._second(x), self.nonlinearity(self._first(x))),\n",
        "            self.partition\n",
        "        )\n",
        "\n",
        "    def inverse(self, y):\n",
        "        \"\"\"Inverse mapping through the layer. Gradients should be turned off for this pass.\"\"\"\n",
        "        return _interleave(\n",
        "            self._first(y),\n",
        "            self.anticoupling_law(self._second(y), self.nonlinearity(self._first(y))),\n",
        "            self.partition\n",
        "        )\n",
        "\n",
        "    def coupling_law(self, a, b):\n",
        "        # (a,b) --> g(a,b)\n",
        "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
        "\n",
        "    def anticoupling_law(self, a, b):\n",
        "        # (a,b) --> g^{-1}(a,b)\n",
        "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
        "\n",
        "\n",
        "class AdditiveCouplingLayer(_BaseCouplingLayer):\n",
        "    \"\"\"Layer with coupling law g(a;b) := a + b.\"\"\"\n",
        "    def coupling_law(self, a, b):\n",
        "        return (a + b)\n",
        "    def anticoupling_law(self, a, b):\n",
        "        return (a - b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ8j5GO8228e"
      },
      "source": [
        "# Implementation of NICE log-likelihood loss.\n",
        "\n",
        "# ===== ===== Loss Function Implementations ===== =====\n",
        "\"\"\"\n",
        "We assume that we final output of the network are components of a multivariate distribution that\n",
        "factorizes, i.e. the output is (y1,y2,...,yK) ~ p(Y) s.t. p(Y) = p_1(Y1) * p_2(Y2) * ... * p_K(YK),\n",
        "with each individual component's prior distribution coming from a standardized family of\n",
        "distributions, i.e. p_i == Gaussian(mu,sigma) for all i in 1..K, or p_i == Logistic(mu,scale).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gaussian_nice_loglkhd(h, diag):\n",
        "    \"\"\"\n",
        "    Definition of log-likelihood function with a Gaussian prior, as in the paper.\n",
        "    \n",
        "    Args:\n",
        "    * h: float tensor of shape (N,D). First dimension is batch dim, second dim consists of components\n",
        "      of a factorized probability distribution.\n",
        "    * diag: scaling diagonal of shape (D,).\n",
        "\n",
        "    Returns:\n",
        "    * loss: torch float tensor of shape (N,).\n",
        "    \"\"\"\n",
        "    # \\sum^D_i s_{ii} - { (1/2) * \\sum^D_i  h_i**2) + (D/2) * log(2\\pi) }\n",
        "    return torch.sum(diag) - (\n",
        "            0.5 * torch.sum(torch.pow(h, 2), dim=1) + h.size(1) * 0.5 * torch.log(torch.tensor(2 * np.pi)))\n",
        "\n",
        "\n",
        "def logistic_nice_loglkhd(h, diag):\n",
        "    \"\"\"\n",
        "    Definition of log-likelihood function with a Logistic prior.\n",
        "    \n",
        "    Same arguments/returns as gaussian_nice_loglkhd.\n",
        "    \"\"\"\n",
        "    # \\sum^D_i s_{ii} - { \\sum^D_i log(exp(h)+1) + torch.log(exp(-h)+1) }\n",
        "    return (torch.sum(diag) - (torch.sum(torch.log1p(torch.exp(h)) + torch.log1p(torch.exp(-h)), dim=1)))\n",
        "\n",
        "\n",
        "# def gumbel_nice_loglkhd(DEVICE, h, diag):\n",
        "def gumbel_nice_loglkhd(DEVICE, h, diag, loc, scale):\n",
        "    \"\"\"\n",
        "    Definition of log-likelihood function with a Gumbel prior.\n",
        "\n",
        "    Args:\n",
        "        h: float tensor of shape (N,D). First dimension is batch dim, second dim consists of components\n",
        "      of a factorized probability distribution.\n",
        "        diag: scaling diagonal of shape (D,).\n",
        "\n",
        "    PDF:exp(-(x - mu) / sigma - exp(-(x - mu) / sigma)) / sigma\n",
        "    It is the formula in both tfp.distributions.Gumbel and torch.distributions\n",
        "    Returns:\n",
        "    * loss: torch float tensor of shape (N,).\n",
        "    \"\"\"\n",
        "    # The location is determined so that mean is 0. When scale = 1, loc = -scale*euler_constant\n",
        "    # loc = torch.ones(h.size()).to(DEVICE) * (-euler_constant)\n",
        "    # loc = torch.zeros(h.size()).to(DEVICE)\n",
        "    # scale = torch.ones(h.size()).to(DEVICE)\n",
        "    loc = torch.tensor(loc, requires_grad=False).to(DEVICE)\n",
        "    # loc = torch.transpose(loc, 0, 1)\n",
        "    loc = loc.repeat(h.size(0), 1)\n",
        "    scale = torch.tensor(scale, requires_grad=False).to(DEVICE)\n",
        "    # scale = torch.transpose(scale, 0, 1)\n",
        "    scale = scale.repeat(h.size(0), 1)\n",
        "    Gumbelloglikelihood = torch.distributions.gumbel.Gumbel(loc, scale).log_prob(h)\n",
        "    return torch.sum(diag) + (torch.sum(Gumbelloglikelihood) / h.size(0))\n",
        "\n",
        "\n",
        "# wrap above loss functions in Modules:\n",
        "class GaussianPriorNICELoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super(GaussianPriorNICELoss, self).__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, fx, diag):\n",
        "        if self.size_average:\n",
        "            return torch.mean(-gaussian_nice_loglkhd(fx, diag))\n",
        "        else:\n",
        "            return torch.sum(-gaussian_nice_loglkhd(fx, diag))\n",
        "\n",
        "\n",
        "class LogisticPriorNICELoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super(LogisticPriorNICELoss, self).__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, fx, diag):\n",
        "        if self.size_average:\n",
        "            return torch.mean(-logistic_nice_loglkhd(fx, diag))\n",
        "        else:\n",
        "            return torch.sum(-logistic_nice_loglkhd(fx, diag))\n",
        "\n",
        "\n",
        "class GumbelPriorNICELoss(nn.Module):\n",
        "    def __init__(self, DEVICE, size_average=True):\n",
        "        super(GumbelPriorNICELoss, self).__init__()\n",
        "        self.size_average = size_average\n",
        "        self.DEVICE = DEVICE\n",
        "\n",
        "    # def forward(self, fx, diag, diag1):\n",
        "    # def forward(self, fx, diag):\n",
        "    def forward(self, fx, diag, loc, scale):\n",
        "        if self.size_average:\n",
        "            return torch.mean(-gumbel_nice_loglkhd(self.DEVICE, fx, diag))\n",
        "            # return torch.mean(-gumbel_nice_loglkhd(self.DEVICE, fx, diag, loc, scale))\n",
        "            # return torch.mean(-gumbel_nice_loglkhd(self.DEVICE, fx, diag, diag1))\n",
        "        else:\n",
        "            # return -gumbel_nice_loglkhd(self.DEVICE, fx, diag)\n",
        "            return -gumbel_nice_loglkhd(self.DEVICE, fx, diag, loc, scale)\n",
        "            # return torch.sum(-gumbel_nice_loglkhd(self.DEVICE, fx, diag, diag1))\n",
        "\n",
        "\n",
        "class regression(nn.Module):\n",
        "    def __init__(self, DEVICE):\n",
        "        super(regression, self).__init__()\n",
        "        self.DEVICE = DEVICE\n",
        "        self.lrloss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    def forward(self, fx, y):\n",
        "        return 0.5 * self.lrloss(fx, y) / fx.size(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkrx4-Gr4Y3D"
      },
      "source": [
        "# l1, l2 regularization\n",
        "def l1_norm(mdl, include_bias=True, device=(torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))):\n",
        "    \"\"\"Compute L1 norm on all the weights of mdl.\"\"\"\n",
        "    if include_bias:\n",
        "        _norm = torch.tensor(0.0, device=device)\n",
        "        for w in mdl.parameters():\n",
        "            _norm = _norm + w.norm(p=1)\n",
        "        return _norm\n",
        "    else:\n",
        "        _norm = torch.tensor(0.0)\n",
        "        for w in mdl.parameters():\n",
        "            if len(w.shape) > 1:\n",
        "                _norm = _norm + w.norm(p=1)\n",
        "        return _norm\n",
        "\n",
        "def l2_norm(mdl, include_bias=True, device=(torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))):\n",
        "    \"\"\"Compute L1 norm on all the weights of mdl.\"\"\"\n",
        "    if include_bias:\n",
        "        _norm = torch.tensor(0.0, device=device)\n",
        "        for w in mdl.parameters():\n",
        "            _norm = _norm + w.norm(p='fro')\n",
        "        return _norm\n",
        "    else:\n",
        "        _norm = torch.tensor(0.0)\n",
        "        for w in mdl.parameters():\n",
        "            if len(w.shape) > 1:\n",
        "                _norm = _norm + w.norm(p='fro')\n",
        "        return _norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-6SyUmG53yC"
      },
      "source": [
        "def val_fMRI(model, Data_valid, CUDA, dataloader_fn, args, validate):\n",
        "    \"\"\"Perform validation on a dataset.\"\"\"\n",
        "    # set model to eval mode (turns batch norm training off)\n",
        "    model.eval()\n",
        "\n",
        "    # build dataloader in eval mode:\n",
        "    dataloader_val = dataloader_fn(Data_valid, batch_size=args.batch_size_val, if_CUDA=CUDA, if_shuffle=False)\n",
        "\n",
        "    # turn gradient-tracking off (for speed) during validation:\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader_val:\n",
        "            scaled_output_val, _ = model(inputs.to(DEVICE))\n",
        "            val_loss = validate(scaled_output_val).item()\n",
        "\n",
        "    # delete dataloader to save memory:\n",
        "    del dataloader_val\n",
        "\n",
        "    # set model back in train mode:\n",
        "    model.train()\n",
        "\n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjKXEFaG7FNR"
      },
      "source": [
        "def test_fMRI(model, Data_test, CUDA, dataloader_fn, Sources, args, input_dim):\n",
        "    \"\"\"Perform validation on a dataset.\"\"\"\n",
        "    # set model to eval mode (turns batch norm training off)\n",
        "    model.eval()\n",
        "\n",
        "    # build dataloader in eval mode:\n",
        "    dataloader_test = dataloader_fn(Data_test, batch_size=args.batch_size_test, if_CUDA=CUDA, if_shuffle=False)\n",
        "\n",
        "    test_z = np.empty((0, input_dim), np.float64)\n",
        "    test_z_scaled = np.empty((0, input_dim), np.float64)\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader_test:\n",
        "            scaled_output_test, output_test = model(inputs.to(DEVICE))\n",
        "            test_z = np.append(test_z, output_test.detach().cpu().numpy(), axis=0)\n",
        "            test_z_scaled = np.append(test_z_scaled, scaled_output_test.detach().cpu().numpy(), axis=0)\n",
        "            # Mean Pearson Correlation\n",
        "            y_targets = Sources.transpose()\n",
        "            corr = np.abs(np.corrcoef(test_z.transpose(), y_targets.transpose()))[:input_dim, input_dim:]\n",
        "            # Permutation correction\n",
        "            maps_permut_index = np.argmax(corr, axis=0)\n",
        "            if (np.sort(maps_permut_index) == np.array(range(input_dim), dtype=np.int)).all():\n",
        "                mpc = np.mean(corr[maps_permut_index, range(input_dim)])\n",
        "                # Mean square error\n",
        "                mse = np.mean(np.square(test_z.transpose() - y_targets.transpose()[maps_permut_index, :]))\n",
        "            else:\n",
        "                mpc = np.mean(corr.diagonal())\n",
        "                # Mean square error\n",
        "                mse = np.mean(np.square(test_z.transpose() - y_targets.transpose()))\n",
        "\n",
        "            # Mean Pearson Correlation, for scaled z\n",
        "            corr_scaled = np.abs(np.corrcoef(test_z_scaled.transpose(), y_targets.transpose()))[:input_dim, input_dim:]\n",
        "            # Permutation correction\n",
        "            scaled_maps_permut_index = np.argmax(corr_scaled, axis=0)\n",
        "            if (np.sort(scaled_maps_permut_index) == np.array(range(input_dim), dtype=np.int)).all():\n",
        "                mpc_scaled = np.mean(corr_scaled[scaled_maps_permut_index, range(input_dim)])\n",
        "                # Mean square error\n",
        "                mse_scaled = np.mean(\n",
        "                    np.square(test_z_scaled.transpose() - y_targets.transpose()[scaled_maps_permut_index, :]))\n",
        "            else:\n",
        "                mpc_scaled = np.mean(corr_scaled.diagonal())\n",
        "                # Mean square error\n",
        "                mse_scaled = np.mean(np.square(test_z_scaled.transpose() - y_targets.transpose()))\n",
        "\n",
        "    del dataloader_test\n",
        "\n",
        "    # set model back in train mode:\n",
        "    model.train()\n",
        "\n",
        "    # return test_reg_results\n",
        "    return corr, maps_permut_index, mpc, mse, mse_scaled, test_z, test_z_scaled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIDZwkfbgfcU"
      },
      "source": [
        "def train(args, Data_train, Data_valid, Data_test, Sources, DEVICE, CUDA):\n",
        "  \"\"\"Construct a NICE model and train over a number of epochs.\"\"\"\n",
        "    # === choose which dataset to build:\n",
        "    # global early_stop_account_start\n",
        "  if args.dataset == 'fMRI':\n",
        "    dataloader_fn = load_fMRI_regression\n",
        "    input_dim = Data_train.shape[0]\n",
        "    # Loss\n",
        "    LOSS_train = np.array([], dtype=np.float64)\n",
        "    LOSS_val = np.array([], dtype=np.float64)\n",
        "    LOSS_val_epoch = np.array([], dtype=np.float64)\n",
        "    LR = np.array([], dtype=np.float64)\n",
        "    # Mean Pearson correlation.-Rui\n",
        "    MPC = np.array([], dtype=np.float64)\n",
        "    # Mean square error.-Rui\n",
        "    MSE = np.array([], dtype=np.float64)\n",
        "    MSE_scaled = np.array([], dtype=np.float64)\n",
        "    params_1 = loadmat(args.param1fp)[args.param1vn].transpose().astype(np.float64)\n",
        "    params_2 = loadmat(args.param2fp)[args.param2vn].transpose().astype(np.float64)\n",
        "\n",
        "  model = NICEModel(input_dim, args.nhidden, args.init_interval, args.dropout_p, args.init_filepath, args.ModelRNG_seed)\n",
        "  if (args.model_path is not None):\n",
        "      assert (os.path.exists(args.model_path)), \"[train] model does not exist at specified location\"\n",
        "      model.load_state_dict(torch.load(args.model_path, map_location='cpu'))\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  # opt = optim.RMSprop(model.parameters(), lr=args.lr, momentum=0.9)\n",
        "  opt = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2), eps=args.eps)\n",
        "\n",
        "  # === choose which loss function to build:\n",
        "  if args.prior == 'logistic':\n",
        "      nice_loss_fn = LogisticPriorNICELoss(size_average=True)\n",
        "  if args.prior == 'Gumbel':\n",
        "      nice_loss_fn = GumbelPriorNICELoss(DEVICE, size_average=False)\n",
        "  if args.prior == 'prior':\n",
        "      nice_loss_fn = GaussianPriorNICELoss(size_average=True)\n",
        "  if args.prior == 'regression':\n",
        "      nice_loss_fn = regression(DEVICE)\n",
        "      \n",
        "  def loss_fn(fx):\n",
        "      \"\"\"Compute NICE loss w/r/t a prior and optional L1 or L2 regularization.\"\"\"\n",
        "      if args.lmbda == 0.0:\n",
        "          return nice_loss_fn(fx, model.scaling_diag, params_1, params_2)\n",
        "          # return nice_loss_fn(fx, model.scaling_diag, model.param1, model.param1)\n",
        "          # return nice_loss_fn(fx, model.scaling_diag)\n",
        "          # return nice_loss_fn(fx, model.scaling_diag, model.scaling_diag1)\n",
        "      else:\n",
        "          # return nice_loss_fn(fx, model.scaling_diag) + args.lmbda * l1_norm(model, include_bias=True)\n",
        "          # return nice_loss_fn(fx, model.scaling_diag) + args.lmbda * l2_norm(model, include_bias=True)\n",
        "          return nice_loss_fn(fx, model.scaling_diag, params_1, params_2) + args.lmbda * l2_norm(model, include_bias=True)\n",
        "          # return nice_loss_fn(fx, model.scaling_diag, params_1, params_2) + args.lmbda * l1_norm(model, include_bias=True)\n",
        "\n",
        "\n",
        "  # Validation metric (MLE)\n",
        "  def validate(fx):\n",
        "      # return nice_loss_fn(fx, model.scaling_diag)\n",
        "      return nice_loss_fn(fx, model.scaling_diag, params_1, params_2)\n",
        "      # return nice_loss_fn(fx, model.scaling_diag, model.param1, model.param1)\n",
        "\n",
        "  # === train over a number of epochs; perform validation after each iteration:\n",
        "  # For convergence check\n",
        "  iter_ind = 0\n",
        "  early_stop = False\n",
        "  # early_stop_account_start = False\n",
        "  early_stop_account = 0\n",
        "  mpc = 0\n",
        "  mse = 0\n",
        "  for epoch in range(args.num_epochs):\n",
        "      if early_stop:\n",
        "          break\n",
        "      dataloader_train = dataloader_fn(Data_train.transpose(), batch_size=args.batch_size_train, if_CUDA=CUDA, if_shuffle=False)\n",
        "      for inputs in dataloader_train:\n",
        "          opt.zero_grad()\n",
        "          scaled_output, _ = model(inputs.to(DEVICE))\n",
        "          loss = loss_fn(scaled_output)\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          LOSS_train = np.append(LOSS_train, loss.item())\n",
        "          # validation for early stopping\n",
        "          loss_val = val_fMRI(model, Data_valid.transpose(), CUDA, dataloader_fn, args, validate)\n",
        "          LOSS_val = np.append(LOSS_val, loss_val)\n",
        "\n",
        "          savemat(args.savedir + 'LOSStrain.mat', {'LOSStrain': LOSS_train})\n",
        "          savemat(args.savedir + 'LOSSval.mat', {'LOSSval': LOSS_val})\n",
        "\n",
        "          print('>>> Epoch = %.0f Iter = %.0f  | Loss_train = %.4f | loss_val = %.4f | MPC = %.4f, MSE = %.4f' %\n",
        "                (epoch, iter_ind, LOSS_train[iter_ind],\n",
        "                  LOSS_val[iter_ind], mpc, mse))\n",
        "          iter_ind += 1\n",
        "\n",
        "      del dataloader_train\n",
        "\n",
        "      # Save learning rates\n",
        "      LR = np.append(LR, opt.param_groups[0]['lr'])\n",
        "      savemat(args.savedir + 'LR_epoch.mat', {'LR_epoch': LR})\n",
        "      # opt.param_groups[0]['lr'] = np.maximum(args.lr * (1.0005 ** (-(epoch+1))), 1e-4)\n",
        "      # Stop the training when validation loss increase after epochs.\n",
        "      LOSS_val_epoch = np.append(LOSS_val_epoch, LOSS_val[iter_ind - 1])\n",
        "      savemat(args.savedir + 'LOSSval_epoch.mat', {'LOSSval_epoch': LOSS_val_epoch})\n",
        "      corr, maps_permut_index, mpc, mse, mse_scaled, test_z, test_z_scaled = \\\n",
        "          test_fMRI(model, Data_test.transpose(), CUDA, dataloader_fn, Sources, args, input_dim)\n",
        "      MPC = np.append(MPC, mpc)\n",
        "      MSE = np.append(MSE, mse)\n",
        "      MSE_scaled = np.append(MSE_scaled, mse_scaled)\n",
        "      savemat(args.savedir + 'MPC.mat', {'MPC': MPC})\n",
        "      savemat(args.savedir + 'MSE.mat', {'MSE': MSE})\n",
        "      savemat(args.savedir + 'MSE_scaled.mat', {'MSE_scaled': MSE_scaled})\n",
        "      if epoch > 0:\n",
        "          if LOSS_val_epoch[epoch] >= LOSS_val_epoch[epoch - 1]:\n",
        "              # early_stop_account_start = True\n",
        "              early_stop_account = early_stop_account + 1\n",
        "              opt.param_groups[0]['lr'] = opt.param_groups[0]['lr'] / 5\n",
        "              savemat(args.savedir + 'epoch' + str(epoch) + '.mat', {'corr': corr,\n",
        "                                                                    'maps_permut_index': maps_permut_index,\n",
        "                                                                    'test_z': test_z,\n",
        "                                                                    'test_z_scaled': test_z_scaled,\n",
        "                                                                    'Iter': iter_ind,\n",
        "                                                                    'mpc': mpc,\n",
        "                                                                    'mse': mse,\n",
        "                                                                    'mse_scaled': mse_scaled})\n",
        "          else:\n",
        "              # early_stop_account_start = False\n",
        "              early_stop_account = 0\n",
        "\n",
        "      # if early_stop_account_start:\n",
        "      #     early_stop_account += 1\n",
        "\n",
        "      # Save the model for every 50 epochs\n",
        "      if (epoch+1) % 50 == 0:\n",
        "          # Calculate the regressed results\n",
        "          _dev = 'cuda' if CUDA else 'cpu'\n",
        "          _fn = \"nice.{0}.h_{1}.p_{2}.e_{3}.epoch_{4}.pt\".format(args.dataset, args.nhidden, args.prior, _dev, epoch+1)\n",
        "          torch.save(model.state_dict(), os.path.join(args.savedir, _fn))\n",
        "          print(\">>> Saved file: {0}\".format(_fn))\n",
        "          savemat(args.savedir + str(epoch+1) + 'EpochResults.mat', {'corr': corr,\n",
        "                                                                     'maps_permut_index': maps_permut_index,\n",
        "                                                                      'test_z': test_z,\n",
        "                                                                      'test_z_scaled': test_z_scaled,\n",
        "                                                                      'Iter': iter_ind,\n",
        "                                                                      'mpc': mpc,\n",
        "                                                                      'mse': mse,\n",
        "                                                                      'mse_scaled': mse_scaled})\n",
        "\n",
        "      # Save the model for early stopping\n",
        "      if early_stop_account == args.early_stop_iter:\n",
        "          early_stop = True\n",
        "          _dev = 'cuda' if CUDA else 'cpu'\n",
        "          _fn = \"nice.{0}.h_{1}.p_{2}.e_{3}.earlystop20epochs.pt\".format(args.dataset, args.nhidden, args.prior, _dev)\n",
        "          torch.save(model.state_dict(), os.path.join(args.savedir, _fn))\n",
        "          print(\">>> Saved file: {0}\".format(_fn))\n",
        "          savemat(args.savedir + 'earlystop.mat', {'corr': corr,\n",
        "                                                    'maps_permut_index': maps_permut_index,\n",
        "                                                    'test_z': test_z,\n",
        "                                                    'test_z_scaled': test_z_scaled,\n",
        "                                                    'Iter': iter_ind,\n",
        "                                                    'mpc': mpc,\n",
        "                                                    'mse': mse,\n",
        "                                                    'mse_scaled': mse_scaled})\n",
        "\n",
        "\n",
        "  print('Training is finished.')\n",
        "  return corr, maps_permut_index, test_z, test_z_scaled, mpc, mse, mse_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2wBGM-mX34H"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # ----- parse training settings:\n",
        "    parser = argparse.ArgumentParser(description=\"Train a fresh NICE model and save.\")\n",
        "    # configuration settings:\n",
        "    parser.add_argument(\"--dataset\", default='fMRI', dest='dataset', choices=('tfd', 'cifar10', 'svhn', 'mnist'),\n",
        "                        help=\"Dataset to train the NICE model on.\")\n",
        "    parser.add_argument(\"--epochs\", dest='num_epochs', default=40000, type=int,\n",
        "                        help=\"Number of epochs to train on. [1500]\")\n",
        "    parser.add_argument(\"--batch_size\", dest=\"batch_size_train\", default=10, type=int,\n",
        "                        help=\"Number of training examples per batch. [16]\")\n",
        "    parser.add_argument(\"--Train_size\", dest=\"size_train\", default=5110, type=int,\n",
        "                        help=\"Number of training examples. [5110]\")\n",
        "    parser.add_argument(\"--Validation_size\", dest=\"size_valid\", default=2558, type=int,\n",
        "                        help=\"Number of validation examples. [2558]\")\n",
        "    parser.add_argument(\"--Signal-to-noise_ratio\", dest='SNR', default=100, type=float,\n",
        "                        help=\"Signal-to-noise ratio in variance. [100]\")\n",
        "    parser.add_argument(\"--savedir\", dest='savedir',\n",
        "                        default=\"./Results_synfMRI/\",\n",
        "                        help=\"Where to save the results and trained models.\")\n",
        "    parser.add_argument(\"--Sourcefilepath\", dest='sourcefp',\n",
        "                        default=\"./SynfMRIspatialmaps/SMnoisysyn_conference.mat\",\n",
        "                        help=\"Source file path.\")\n",
        "    parser.add_argument(\"--Sourcevariablename\", dest='sourcevn',\n",
        "                        default=\"SM\",\n",
        "                        help=\"Source variable name.\")\n",
        "    parser.add_argument(\"--param1filepath\", dest='param1fp',\n",
        "                        default=\"./DistributionParams/SM_params_locat.mat\",\n",
        "                        help=\"Parameter 1 file path.\")\n",
        "    parser.add_argument(\"--param1variablename\", dest='param1vn',\n",
        "                        default=\"locat_param\",\n",
        "                        help=\"Parameter 1 variable name.\")\n",
        "    parser.add_argument(\"--param2filepath\", dest='param2fp',\n",
        "                        default=\"./DistributionParams/SM_params_scale.mat\",\n",
        "                        help=\"Parameter 2 file path.\")\n",
        "    parser.add_argument(\"--param2variablename\", dest='param2vn',\n",
        "                        default=\"scale_param\",\n",
        "                        help=\"Parameter 2 variable name.\")\n",
        "    parser.add_argument(\"--initialization_path\", dest='init_filepath',\n",
        "                        default=\"./\",\n",
        "                        help=\"Where to load the pretrained model parameters.\")\n",
        "\n",
        "    # validation and test:\n",
        "    parser.add_argument(\"--val_batch_size\", dest=\"batch_size_val\", default=2558, type=int,\n",
        "                        help=\"Number of validation examples per batch. [16]\")\n",
        "    parser.add_argument(\"--test_batch_size\", dest=\"batch_size_test\", default=7668, type=int,\n",
        "                        help=\"Number of test examples per batch. [16]\")\n",
        "    parser.add_argument(\"--early_stop_iteration\", dest=\"early_stop_iter\", default=3, type=int,\n",
        "                        help=\"Number of iterations for early stopping. [16]\")\n",
        "\n",
        "    # model settings:\n",
        "    parser.add_argument(\"--nonlinearity_hiddens\", dest='nhidden', default=2, type=int,\n",
        "                        help=\"Hidden size of inner layers of nonlinearity. [1000]\")\n",
        "    parser.add_argument(\"--nonlinearity_dropout\", dest='dropout_p', default=0.0, type=float,\n",
        "                        help=\"The dropout probability in each layer (except scaling layer). [0.8]\")\n",
        "    parser.add_argument(\"--prior\", choices=('logistic', 'Gumbel', 'prior', 'regression'), default='Gumbel',\n",
        "                        help=\"Prior distribution of latent space components. [logistic]\")\n",
        "    parser.add_argument(\"--model_path\", dest='model_path', default=None, type=str,\n",
        "                        help=\"Continue from pretrained model. [None]\")\n",
        "    parser.add_argument(\"--uniform_init_interval\", dest='init_interval', default=1e-2, type=float,\n",
        "                        help=\"The interval of uniform initialization. [0.01]\")\n",
        "\n",
        "    # optimization settings:\n",
        "    parser.add_argument(\"--lr\", default=1e-3, dest='lr', type=float,\n",
        "                        help=\"Learning rate for ADAM optimizer. [0.001]\")\n",
        "    parser.add_argument(\"--beta1\", default=0.9, dest='beta1', type=float,\n",
        "                        help=\"Momentum for ADAM optimizer. [0.9]\")\n",
        "    parser.add_argument(\"--beta2\", default=0.999, dest='beta2', type=float,\n",
        "                        help=\"Beta2 for ADAM optimizer. [0.01]\")\n",
        "    parser.add_argument(\"--eps\", default=1e-8, dest='eps', type=float,\n",
        "                        help=\"Epsilon for ADAM optimizer. [0.0001]\")\n",
        "    parser.add_argument(\"--lambda\", default=0.0, dest='lmbda', type=float,\n",
        "                        help=\"L1 weight decay coefficient. [0.0]\")\n",
        "    parser.add_argument(\"--DataRNGseed\", dest=\"DataRNG_seed\", default=1, type=int,\n",
        "                        help=\"Data random number generator seed. [1]\")\n",
        "    parser.add_argument(\"--ModelRNGseed\", dest=\"ModelRNG_seed\", default=1, type=int,\n",
        "                        help=\"Model random number generator seed. [1]\")\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "718wvRSXbXdV"
      },
      "source": [
        "    # ----- run training loop over several epochs & save models for each epoch:\n",
        "    \n",
        "    # Data type\n",
        "    torch.set_default_dtype(torch.double)\n",
        "    \n",
        "    # Data generation and split: N_source x N_sample\n",
        "    # Sources = Gen_Source(loc_filename=args.param1fp, loc_varname=args.param1vn, \n",
        "                        #  scale_filename=args.param2fp, sacle_varname=args.param2vn,\n",
        "                        #  N_sample=args.size_train+args.size_valid, seed_gs=args.DataRNG_seed)\n",
        "    Sources = loadmat(args.sourcefp)[args.sourcevn].astype(np.float64)\n",
        "    Data = Gen_Mixture(Sources, ratio=1)\n",
        "    Data = Add_Noise(Data, args.SNR, args.DataRNG_seed)\n",
        "    Data_train, Data_valid = Train_Valid_Split(Data, args.size_train, args.size_valid, args.DataRNG_seed)\n",
        "\n",
        "    # CUDA check\n",
        "    if torch.cuda.is_available():\n",
        "      DEVICE = torch.device('cuda:0')\n",
        "      CUDA = True\n",
        "    else:\n",
        "      DEVICE = torch.device('cpu')\n",
        "      CUDA = False\n",
        "    \n",
        "    print('Training device:'+DEVICE.type)\n",
        "    corr, maps_permut_index, test_z, test_z_scaled, mpc, mse, mse_scaled = train(args, Data_train, Data_valid, Data, Sources, DEVICE, CUDA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kp9p0bTrTdF"
      },
      "source": [
        "# Check the correlation (estimates x ground truth)\n",
        "plt.imshow(corr[maps_permut_index,:])\n",
        "plt.colorbar()\n",
        "print(np.diag(corr[maps_permut_index,:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DqtQBVfsQqu"
      },
      "source": [
        "# Compare the estimated component histogram with the ground truth (#1 component as an example.) \n",
        "test_z_scaled = test_z_scaled[:,maps_permut_index] # Permutation correction on estimates\n",
        "Ind_source_tocompare = 0\n",
        "loc = loadmat(args.param1fp)[args.param1vn].astype(np.float64)\n",
        "scale = loadmat(args.param2fp)[args.param2vn].astype(np.float64)\n",
        "n_out, bins_out, patches_out = plt.hist(test_z_scaled[:,Ind_source_tocompare], density=True, bins='auto')\n",
        "y = scistats.gumbel_r.pdf(bins_out, loc[Ind_source_tocompare], scale[Ind_source_tocompare])\n",
        "plt.plot(bins_out,y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}